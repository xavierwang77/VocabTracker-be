#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Preplyè¯æ±‡æµ‹è¯•ç»“æœåˆ†æè„šæœ¬

åŠŸèƒ½è¯´æ˜:
- è¯»å–preply_resultsç›®å½•ä¸­çš„æµ‹è¯•ç»“æœæ–‡ä»¶
- åˆ†æç”¨æˆ·è®¤è¯†çš„å•è¯åœ¨å„è¯æ±‡è¡¨(CET4/CET6/è€ƒç ”/ä¸“å››/ä¸“å…«)ä¸­çš„åˆ†å¸ƒ
- åŸºäºè¯æ±‡è¡¨åˆ†å¸ƒè®¡ç®—ç”¨æˆ·è¯æ±‡é‡
- ä¸Preplyå®˜æ–¹ç»“æœå¯¹æ¯”ï¼Œè®¡ç®—å·®å¼‚ç‡

ä½¿ç”¨æ–¹æ³•:
    python analyze_vocab_results.py
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from app.core.config import Settings
from app.models.vocabulary import (
    CET4Vocabulary, CET6Vocabulary, KaoyanVocabulary, 
    Level4Vocabulary, Level8Vocabulary
)
from app.service.vocabulary_service import VocabularyEstimateService

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vocab_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class VocabResultAnalyzer:
    """
    è¯æ±‡æµ‹è¯•ç»“æœåˆ†æå™¨
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        """
        self.settings = Settings()
        self.engine = None
        self.session = None
        self._setup_database()
        
        # è¯æ±‡è¡¨ä¼˜å…ˆçº§æ˜ å°„ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        self.vocab_priority = {
            'cet4': 1,
            'cet6': 2, 
            'kaoyan': 3,
            'level4': 4,
            'level8': 5
        }
        
        # è¯æ±‡è¡¨æ¨¡å‹æ˜ å°„
        self.vocab_models = {
            'cet4': CET4Vocabulary,
            'cet6': CET6Vocabulary,
            'kaoyan': KaoyanVocabulary,
            'level4': Level4Vocabulary,
            'level8': Level8Vocabulary
        }
        
        # è¯æ±‡è¡¨ä¸­æ–‡åç§°
        self.vocab_names = {
            'cet4': 'CET4',
            'cet6': 'CET6',
            'kaoyan': 'è€ƒç ”',
            'level4': 'ä¸“å››',
            'level8': 'ä¸“å…«'
        }
    
    def _setup_database(self):
        """
        è®¾ç½®æ•°æ®åº“è¿æ¥
        """
        try:
            # æ„å»ºæ•°æ®åº“è¿æ¥URL
            db_url = (
                f"postgresql://{self.settings.database_user}:"
                f"{self.settings.database_password}@"
                f"{self.settings.database_host}:"
                f"{self.settings.database_port}/"
                f"{self.settings.database_name}"
            )
            
            self.engine = create_engine(db_url, echo=False)
            SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
            self.session = SessionLocal()
            
            logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def get_test_result_files(self) -> List[str]:
        """
        è·å–preply_resultsç›®å½•ä¸­çš„æ‰€æœ‰æµ‹è¯•ç»“æœæ–‡ä»¶
        
        Returns:
            List[str]: æµ‹è¯•ç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        results_dir = project_root / 'preply_results'
        if not results_dir.exists():
            logger.error(f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
            return []
        
        json_files = list(results_dir.glob('vocab_test_result_*.json'))
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯•ç»“æœæ–‡ä»¶")
        
        return [str(f) for f in json_files]
    
    def load_test_result(self, file_path: str) -> Dict[str, Any]:
        """
        åŠ è½½æµ‹è¯•ç»“æœæ–‡ä»¶
        
        Args:
            file_path: æµ‹è¯•ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[str, Any]: æµ‹è¯•ç»“æœæ•°æ®
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½æµ‹è¯•ç»“æœ: {file_path}")
            return data
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•ç»“æœæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {}
    
    def extract_known_words(self, test_data: Dict[str, Any]) -> List[str]:
        """
        æå–ç”¨æˆ·è®¤è¯†çš„å•è¯åˆ—è¡¨
        
        Args:
            test_data: æµ‹è¯•ç»“æœæ•°æ®
            
        Returns:
            List[str]: ç”¨æˆ·è®¤è¯†çš„å•è¯åˆ—è¡¨
        """
        known_words = []
        
        for round_data in test_data.get('rounds', []):
            for word_info in round_data.get('words', []):
                if word_info.get('known', False):
                    known_words.append(word_info.get('word', '').lower())
        
        # å»é‡
        known_words = list(set(known_words))
        logger.info(f"æå–åˆ° {len(known_words)} ä¸ªè®¤è¯†çš„å•è¯")
        
        return known_words
    
    def analyze_word_distribution(self, known_words: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        åˆ†æå•è¯åœ¨å„è¯æ±‡è¡¨ä¸­çš„åˆ†å¸ƒ
        
        Args:
            known_words: ç”¨æˆ·è®¤è¯†çš„å•è¯åˆ—è¡¨
            
        Returns:
            Dict[str, Dict[str, Any]]: å„è¯æ±‡è¡¨çš„åˆ†æç»“æœ
        """
        distribution = {}
        word_levels = {}  # è®°å½•æ¯ä¸ªå•è¯çš„æœ€é«˜ç­‰çº§
        
        # æŸ¥è¯¢æ¯ä¸ªè¯æ±‡è¡¨
        for vocab_type, model_class in self.vocab_models.items():
            try:
                # æŸ¥è¯¢è¯¥è¯æ±‡è¡¨ä¸­å­˜åœ¨çš„å•è¯
                found_words = self.session.query(model_class.head_word).filter(
                    model_class.head_word.in_(known_words)
                ).all()
                
                found_word_list = [word[0].lower() for word in found_words]
                
                # æ›´æ–°å•è¯ç­‰çº§æ˜ å°„
                for word in found_word_list:
                    current_priority = self.vocab_priority[vocab_type]
                    if word not in word_levels or word_levels[word]['priority'] < current_priority:
                        word_levels[word] = {
                            'level': vocab_type,
                            'priority': current_priority
                        }
                
                distribution[vocab_type] = {
                    'found_words': found_word_list,
                    'count': len(found_word_list),
                    'total_tested': len(known_words)
                }
                
                logger.info(f"{self.vocab_names[vocab_type]}è¯æ±‡è¡¨: æ‰¾åˆ° {len(found_word_list)} ä¸ªå•è¯")
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢{vocab_type}è¯æ±‡è¡¨æ—¶å‡ºé”™: {e}")
                distribution[vocab_type] = {
                    'found_words': [],
                    'count': 0,
                    'total_tested': len(known_words)
                }
        
        # ç»Ÿè®¡æŒ‰æœ€é«˜ç­‰çº§åˆ†ç±»çš„å•è¯æ•°é‡
        level_counts = {level: 0 for level in self.vocab_priority.keys()}
        for word_info in word_levels.values():
            level_counts[word_info['level']] += 1
        
        # æ·»åŠ ç­‰çº§ç»Ÿè®¡ä¿¡æ¯
        for vocab_type in distribution.keys():
            distribution[vocab_type]['highest_level_count'] = level_counts[vocab_type]
        
        return distribution
    
    def calculate_vocabulary_estimate(self, distribution: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        åŸºäºè¯æ±‡è¡¨åˆ†å¸ƒè®¡ç®—è¯æ±‡é‡ä¼°ç®—
        
        Args:
            distribution: è¯æ±‡è¡¨åˆ†å¸ƒåˆ†æç»“æœ
            
        Returns:
            Dict[str, Any]: è¯æ±‡é‡ä¼°ç®—ç»“æœ
        """
        from app.service.vocabulary_service import VocabularyTestResult, VocabularyEstimateRequest
        
        # æ„å»ºæµ‹è¯•ç»“æœ
        test_results = {}
        for vocab_type, data in distribution.items():
            if data['total_tested'] > 0:
                test_results[vocab_type] = VocabularyTestResult(
                    known=data['highest_level_count'],  # ä½¿ç”¨æœ€é«˜ç­‰çº§è®¡æ•°
                    total=data['total_tested']
                )
            else:
                test_results[vocab_type] = VocabularyTestResult(known=0, total=0)
        
        # åˆ›å»ºä¼°ç®—è¯·æ±‚
        request = VocabularyEstimateRequest(
            cet4=test_results['cet4'],
            cet6=test_results['cet6'],
            kaoyan=test_results['kaoyan'],
            level4=test_results['level4'],
            level8=test_results['level8']
        )
        
        # è®¡ç®—ä¼°ç®—ç»“æœ
        estimate_response = VocabularyEstimateService.estimate_vocabulary(request)
        
        return {
            'estimated_vocabulary': estimate_response.estimated_vocabulary,
            'breakdown': estimate_response.breakdown,
            'confidence_level': estimate_response.confidence_level,
            'recommendations': estimate_response.recommendations
        }
    
    def calculate_difference_rate(self, our_estimate: int, preply_result: str) -> Tuple[float, str]:
        """
        è®¡ç®—æˆ‘ä»¬çš„ä¼°ç®—ä¸Preplyç»“æœçš„å·®å¼‚ç‡
        
        Args:
            our_estimate: æˆ‘ä»¬çš„è¯æ±‡é‡ä¼°ç®—
            preply_result: Preplyçš„è¯æ±‡é‡ç»“æœ
            
        Returns:
            Tuple[float, str]: (å·®å¼‚ç‡, å·®å¼‚æè¿°)
        """
        try:
            preply_vocab = int(preply_result)
            if preply_vocab == 0:
                return 0.0, "Preplyç»“æœä¸º0ï¼Œæ— æ³•è®¡ç®—å·®å¼‚ç‡"
            
            difference_rate = abs(our_estimate - preply_vocab) / preply_vocab * 100
            
            if our_estimate > preply_vocab:
                direction = "é«˜ä¼°"
            elif our_estimate < preply_vocab:
                direction = "ä½ä¼°"
            else:
                direction = "ä¸€è‡´"
            
            return difference_rate, f"{direction} {difference_rate:.1f}%"
            
        except (ValueError, TypeError):
            return 0.0, "Preplyç»“æœæ ¼å¼é”™è¯¯ï¼Œæ— æ³•è®¡ç®—å·®å¼‚ç‡"
    
    def analyze_single_result(self, file_path: str) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæµ‹è¯•ç»“æœæ–‡ä»¶
        
        Args:
            file_path: æµ‹è¯•ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_result(file_path)
        if not test_data:
            return {}
        
        # æå–ç”¨æˆ·ä¿¡æ¯
        file_name = os.path.basename(file_path)
        user_id = file_name.replace('vocab_test_result_', '').replace('.json', '')
        
        # æå–è®¤è¯†çš„å•è¯
        known_words = self.extract_known_words(test_data)
        
        # åˆ†æè¯æ±‡è¡¨åˆ†å¸ƒ
        distribution = self.analyze_word_distribution(known_words)
        
        # è®¡ç®—è¯æ±‡é‡ä¼°ç®—
        our_estimate = self.calculate_vocabulary_estimate(distribution)
        
        # è·å–Preplyç»“æœ
        preply_result = test_data.get('final_vocab_size', '0')
        
        # è®¡ç®—å·®å¼‚ç‡
        diff_rate, diff_desc = self.calculate_difference_rate(
            our_estimate['estimated_vocabulary'], 
            preply_result
        )
        
        return {
            'user_id': user_id,
            'file_path': file_path,
            'known_words_count': len(known_words),
            'distribution': distribution,
            'our_estimate': our_estimate,
            'preply_result': preply_result,
            'difference_rate': diff_rate,
            'difference_description': diff_desc,
            'test_summary': test_data.get('summary', {})
        }
    
    def analyze_all_results(self) -> List[Dict[str, Any]]:
        """
        åˆ†ææ‰€æœ‰æµ‹è¯•ç»“æœæ–‡ä»¶
        
        Returns:
            List[Dict[str, Any]]: æ‰€æœ‰ç”¨æˆ·çš„åˆ†æç»“æœ
        """
        result_files = self.get_test_result_files()
        if not result_files:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
            return []
        
        all_results = []
        
        for file_path in result_files:
            logger.info(f"æ­£åœ¨åˆ†æ: {file_path}")
            result = self.analyze_single_result(file_path)
            if result:
                all_results.append(result)
        
        return all_results
    
    def generate_summary_report(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        
        Args:
            all_results: æ‰€æœ‰ç”¨æˆ·çš„åˆ†æç»“æœ
            
        Returns:
            Dict[str, Any]: æ±‡æ€»æŠ¥å‘Š
        """
        if not all_results:
            return {}
        
        total_users = len(all_results)
        total_diff_rates = [r['difference_rate'] for r in all_results if r['difference_rate'] > 0]
        
        avg_diff_rate = sum(total_diff_rates) / len(total_diff_rates) if total_diff_rates else 0
        
        # ç»Ÿè®¡å„è¯æ±‡è¡¨çš„å¹³å‡å‘½ä¸­ç‡
        vocab_stats = {}
        for vocab_type in self.vocab_priority.keys():
            hit_rates = []
            for result in all_results:
                dist = result['distribution'].get(vocab_type, {})
                if dist.get('total_tested', 0) > 0:
                    hit_rate = dist.get('highest_level_count', 0) / dist['total_tested'] * 100
                    hit_rates.append(hit_rate)
            
            vocab_stats[vocab_type] = {
                'name': self.vocab_names[vocab_type],
                'avg_hit_rate': sum(hit_rates) / len(hit_rates) if hit_rates else 0,
                'users_tested': len(hit_rates)
            }
        
        return {
            'total_users': total_users,
            'average_difference_rate': avg_diff_rate,
            'vocab_stats': vocab_stats,
            'analysis_time': datetime.now().isoformat()
        }
    
    def save_analysis_report(self, all_results: List[Dict[str, Any]], summary: Dict[str, Any]):
        """
        ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            all_results: æ‰€æœ‰ç”¨æˆ·çš„åˆ†æç»“æœ
            summary: æ±‡æ€»æŠ¥å‘Š
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = project_root / f'preply_results_analysis_report_{timestamp}.json'
        
        report_data = {
            'summary': summary,
            'detailed_results': all_results
        }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
    
    def print_summary_report(self, summary: Dict[str, Any]):
        """
        æ‰“å°æ±‡æ€»æŠ¥å‘Š
        
        Args:
            summary: æ±‡æ€»æŠ¥å‘Šæ•°æ®
        """
        print("\n" + "=" * 80)
        print("ğŸ“Š Preplyè¯æ±‡æµ‹è¯•ç»“æœåˆ†ææ±‡æ€»æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   - åˆ†æç”¨æˆ·æ•°: {summary.get('total_users', 0)}")
        print(f"   - å¹³å‡å·®å¼‚ç‡: {summary.get('average_difference_rate', 0):.1f}%")
        
        print(f"\nğŸ“š å„è¯æ±‡è¡¨å‘½ä¸­ç‡ç»Ÿè®¡:")
        vocab_stats = summary.get('vocab_stats', {})
        for vocab_type, stats in vocab_stats.items():
            print(f"   - {stats['name']}: {stats['avg_hit_rate']:.1f}% (æµ‹è¯•ç”¨æˆ·: {stats['users_tested']})")
        
        print(f"\nâ° åˆ†ææ—¶é—´: {summary.get('analysis_time', '')}")
        print("=" * 80)
    
    def close(self):
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        if self.session:
            self.session.close()
        if self.engine:
            self.engine.dispose()
        logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("\nğŸš€ å¼€å§‹Preplyè¯æ±‡æµ‹è¯•ç»“æœåˆ†æ...")
    
    analyzer = None
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = VocabResultAnalyzer()
        
        # åˆ†ææ‰€æœ‰ç»“æœ
        all_results = analyzer.analyze_all_results()
        
        if not all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•ç»“æœæ–‡ä»¶")
            return
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = analyzer.generate_summary_report(all_results)
        
        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        analyzer.print_summary_report(summary)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        analyzer.save_analysis_report(all_results, summary)
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†åˆ†æè¿‡ç¨‹")
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
    finally:
        if analyzer:
            analyzer.close()


if __name__ == "__main__":
    main()